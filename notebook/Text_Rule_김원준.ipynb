{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get add the Base directory\n",
    "BASE_DIR = str(Path().cwd().parent.resolve())\n",
    "sys.path.insert(0, BASE_DIR)\n",
    "TEXT_COLS = [1,5,6,7,8,17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{BASE_DIR}/data/fake_job_postings.csv', engine = 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_fake = df[df['fraudulent'] == 1]\n",
    "only_real = df[df['fraudulent'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_fake_text = only_fake.iloc[:,TEXT_COLS].copy(deep=True)\n",
    "only_real_text = only_real.iloc[:,TEXT_COLS].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine(df):\n",
    "    for col in df.columns:\n",
    "        if col == 'fraudulent':\n",
    "            continue\n",
    "\n",
    "        # replace &amp; to &\n",
    "        df[col] = df[col].str.replace(r'&amp;', '&')\n",
    "        \n",
    "        # replace &nbsp; to space\n",
    "        df[col] = df[col].str.replace(r'&nbsp;', ' ')\n",
    "        \n",
    "        # replace &lt; to <\n",
    "        df[col] = df[col].str.replace(r'&lt;', '<')\n",
    "        \n",
    "        # replace &gt; to >\n",
    "        df[col] = df[col].str.replace(r'&gt;', '>')\n",
    "        \n",
    "        # replace &quot; to \"\n",
    "        df[col] = df[col].str.replace(r'&quot;', '\"')\n",
    "        \n",
    "        # replace \\u00a0 to space\n",
    "        df[col] = df[col].str.replace(r'\\u00a0', ' ')\n",
    "    \n",
    "\n",
    "    df = df.fillna('None')\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_only_fake_text = refine(only_fake_text)\n",
    "refined_only_real_text = refine(only_real_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract unique keywords from text using tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def get_unique_keywords(df, col, top_k=20, d_type='real', ngram=(1,1)):\n",
    "    tfidf = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        ngram_range=ngram,\n",
    "        )\n",
    "    tfidf.fit(df[col])\n",
    "    terms = tfidf.get_feature_names_out().tolist()\n",
    "    score = tfidf.transform(df[col]).toarray().flatten().tolist()\n",
    "    \n",
    "    data = [(t, s) for t, s in zip(terms, score)]\n",
    "    sorted_iter = sorted(data, key=lambda x: x[1], reverse=True)\n",
    "    sorted_result = {}\n",
    "    \n",
    "    for k, v in sorted_iter:\n",
    "        sorted_result[k] = v\n",
    "    \n",
    "    path_dir = f'{BASE_DIR}/data/tf_idf'\n",
    "    \n",
    "    if not os.path.isdir(path_dir):\n",
    "        os.mkdir(path_dir)\n",
    "    \n",
    "    with open(f'{path_dir}/{d_type}_{col}_{ngram}.json', 'w') as f:\n",
    "        json.dump(sorted_result, f, indent=4)\n",
    "        \n",
    "    return list(sorted_result)[:top_k]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====title=====\n",
      "['ic', 'technician', '000', '100', '130', '15', '150', '175', '1781', '1843', '19', '1970', '20', '200', '2020', '2022', '2048', '2053', '2131', '2141']\n",
      "\n",
      "\n",
      "\n",
      "=====company_profile=====\n",
      "['candidates', 'significant', 'refined', 'resources', 'bonus', 'referral', 'signing', 'anyperk', 'automatically', 'behalf', 'cleaning', 'click', 'corporate', 'directly', 'discounts', 'enlarge', 'expenditures', 'formthank', 'forward', 'granted']\n",
      "\n",
      "\n",
      "\n",
      "=====description=====\n",
      "['equipment', 'performs', 'maintenance', 'members', 'required', 'installs', 'troubleshoots', 'identifies', 'motor', 'control', 'instrumentation', 'follows', 'tasks', 'plant', 'power', 'electrical', 'environmental', 'team', 'record', 'controls']\n",
      "\n",
      "\n",
      "\n",
      "=====requirements=====\n",
      "['demonstrated', 'equipment', 'plant', 'systems', 'including', 'electrical', 'power', 'ability', 'control', 'environment', 'qualificationsknowledge', 'basics', 'calibrate', 'cem', 'controllers', 'emissions', 'generators', 'programmable', 'transformers', 'analytic']\n",
      "\n",
      "\n",
      "\n",
      "=====benefits=====\n",
      "['careers', 'benefitswhat', 'renound', 'goalsqualified', 'phone_70128aad0c118273b0c2198a08d528591b932924e165b6a8d1272a6f9e2763d1', 'company', 'companysignificant', 'cultureworld', 'fundannual', 'increasesannual', 'invest', 'overall', 'package100', 'packageannual', 'sound', 'structureopportunity', 'leverage', 'increases', 'living', 'promote']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in refined_only_fake_text.columns[:-1]:\n",
    "    print(f'====={c}=====')\n",
    "    print(get_unique_keywords(refined_only_fake_text, c, 20, d_type='fake', ngram=(1,1)))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====title=====\n",
      "[('intern', 0.7775118106326152), ('marketing', 0.6288683362412136), ('00', 0.0), ('000', 0.0), ('02', 0.0), ('0dq', 0.0), ('0rg', 0.0), ('0tz', 0.0), ('10', 0.0), ('100', 0.0), ('100k', 0.0), ('1099', 0.0), ('10k', 0.0), ('10x', 0.0), ('11', 0.0), ('12', 0.0), ('12hr', 0.0), ('13', 0.0), ('13th', 0.0), ('14', 0.0)]\n",
      "\n",
      "\n",
      "\n",
      "=====company_profile=====\n",
      "[('food', 0.2840718248529412), ('food52', 0.259000944091012), ('cooks', 0.2533007527643997), ('cooking', 0.252553696071854), ('connect', 0.18780744507929095), ('york', 0.15428606971489003), ('new', 0.15254877778917614), ('home', 0.15058764845383665), ('batali', 0.129500472045506), ('beard', 0.129500472045506), ('contributors', 0.129500472045506), ('danny', 0.129500472045506), ('gwyneth', 0.129500472045506), ('iacp', 0.129500472045506), ('mario', 0.129500472045506), ('meyer', 0.129500472045506), ('npr', 0.129500472045506), ('paltrow', 0.129500472045506), ('pando', 0.129500472045506), ('random', 0.129500472045506)]\n",
      "\n",
      "\n",
      "\n",
      "=====description=====\n",
      "[('food52', 0.37028043979670333), ('affiliate', 0.34090065414727244), ('inquiriessupporting', 0.16372199311942046), ('meetingsworking', 0.16372199311942046), ('neededhelping', 0.16372199311942046), ('programassisting', 0.16372199311942046), ('systemsresearching', 0.16372199311942046), ('repackaging', 0.1598169867156216), ('reproducing', 0.1598169867156216), ('sitesupporting', 0.1566263641650804), ('huffington', 0.14601900320585243), ('buzzfeed', 0.1444963063483958), ('beard', 0.1355568710822524), ('developers', 0.13175972976926575), ('crowd', 0.13096550320789752), ('james', 0.13030504843971571), ('recipe', 0.12905451840953716), ('content', 0.127899502710143), ('affiliates', 0.1278874210725913), ('provisions', 0.1273318196693666)]\n",
      "\n",
      "\n",
      "\n",
      "=====requirements=====\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m refined_only_real_text\u001b[39m.\u001b[39mcolumns[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[0;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m=====\u001b[39m\u001b[39m{\u001b[39;00mc\u001b[39m}\u001b[39;00m\u001b[39m=====\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mprint\u001b[39m(get_unique_keywords(refined_only_real_text, c, \u001b[39m20\u001b[39;49m))\n\u001b[0;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m, in \u001b[0;36mget_unique_keywords\u001b[1;34m(df, col, top_k)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_unique_keywords\u001b[39m(df, col, top_k\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m):\n\u001b[0;32m      5\u001b[0m     tfidf \u001b[39m=\u001b[39m TfidfVectorizer(\n\u001b[0;32m      6\u001b[0m         stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m         ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m),\n\u001b[0;32m      8\u001b[0m         )\n\u001b[1;32m----> 9\u001b[0m     tfidf\u001b[39m.\u001b[39;49mfit(df[col])\n\u001b[0;32m     10\u001b[0m     terms \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39mget_feature_names_out()\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     11\u001b[0m     score \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39mtransform(df[col])\u001b[39m.\u001b[39mtoarray()\u001b[39m.\u001b[39mflatten()\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\glist\\PycharmProjects\\BigDataAnalysis-HYU-IE\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2103\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2096\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_for_unused_params()\n\u001b[0;32m   2097\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2098\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2099\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2100\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2101\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2102\u001b[0m )\n\u001b[1;32m-> 2103\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\glist\\PycharmProjects\\BigDataAnalysis-HYU-IE\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\glist\\PycharmProjects\\BigDataAnalysis-HYU-IE\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1287\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1284\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m     j_indices\u001b[39m.\u001b[39mextend(feature_counter\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m-> 1287\u001b[0m     values\u001b[39m.\u001b[39;49mextend(feature_counter\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m   1288\u001b[0m     indptr\u001b[39m.\u001b[39mappend(\u001b[39mlen\u001b[39m(j_indices))\n\u001b[0;32m   1290\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fixed_vocab:\n\u001b[0;32m   1291\u001b[0m     \u001b[39m# disable defaultdict behaviour\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for c in refined_only_real_text.columns[:-1]:\n",
    "    print(f'====={c}=====')\n",
    "    print(get_unique_keywords(refined_only_real_text, c, 20))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_by = 100\n",
    "number_of_parts = len(only_fake_text)//div_by + 1\n",
    "\n",
    "for i in range(number_of_parts):\n",
    "    only_fake_text.iloc[:i*div_by, :].to_json(f'{BASE_DIR}/data/only_fake_text_{i}.json', indent=4, orient='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
