{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get add the Base directory\n",
    "BASE_DIR = str(Path().cwd().parent.resolve())\n",
    "sys.path.insert(0, BASE_DIR)\n",
    "TEXT_COLS = [1,5,6,7,8,17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{BASE_DIR}/data/fake_job_postings.csv', engine = 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_fake = df[df['fraudulent'] == 1]\n",
    "only_real = df[df['fraudulent'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_fake_text = only_fake.iloc[:,TEXT_COLS].copy(deep=True)\n",
    "only_real_text = only_real.iloc[:,TEXT_COLS].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def refine(df):\n",
    "    for col in df.columns:\n",
    "        if col == 'fraudulent':\n",
    "            continue\n",
    "\n",
    "        # replace &amp; to &\n",
    "        df[col] = df[col].str.replace(r'&amp;', '&')\n",
    "        \n",
    "        # replace &nbsp; to space\n",
    "        df[col] = df[col].str.replace(r'&nbsp;', ' ')\n",
    "        \n",
    "        # replace &lt; to <\n",
    "        df[col] = df[col].str.replace(r'&lt;', '<')\n",
    "        \n",
    "        # replace &gt; to >\n",
    "        df[col] = df[col].str.replace(r'&gt;', '>')\n",
    "        \n",
    "        # replace &quot; to \"\n",
    "        df[col] = df[col].str.replace(r'&quot;', '\"')\n",
    "        \n",
    "        # replace \\u00a0 to space\n",
    "        df[col] = df[col].str.replace(r'\\u00a0', ' ')\n",
    "        \n",
    "        df[col] = df[col].fillna('None')\n",
    "        \n",
    "        # stop word remove\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        df[col] = df[col].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "        \n",
    "        # lemmatize\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        df[col] = df[col].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_only_fake_text = refine(only_fake_text)\n",
    "refined_only_real_text = refine(only_real_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract unique keywords from text using tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def get_unique_keywords(df, col, top_k=20, d_type='real', ngram=(1,1)):\n",
    "    tfidf = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        ngram_range=ngram,\n",
    "        )\n",
    "    tfidf.fit(df[col])\n",
    "    terms = tfidf.get_feature_names_out().tolist()\n",
    "    score = tfidf.transform(df[col]).toarray().flatten().tolist()\n",
    "    \n",
    "    data = [(t, s) for t, s in zip(terms, score)]\n",
    "    sorted_iter = sorted(data, key=lambda x: x[1], reverse=True)\n",
    "    sorted_result = {}\n",
    "    \n",
    "    for k, v in sorted_iter:\n",
    "        sorted_result[k] = v\n",
    "    \n",
    "    path_dir = f'{BASE_DIR}/data/tf_idf'\n",
    "    \n",
    "    if not os.path.isdir(path_dir):\n",
    "        os.mkdir(path_dir)\n",
    "    \n",
    "    with open(f'{path_dir}/{d_type}_{col}_{ngram}.json', 'w') as f:\n",
    "        json.dump(sorted_result, f, indent=4)\n",
    "        \n",
    "    return list(sorted_result)[:top_k]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====title=====\n",
      "['ic', 'technician', '000', '100', '130', '15', '150', '175', '1781', '1843', '19', '1970', '20', '200', '2020', '2022', '2048', '2053', '2131', '2141']\n",
      "\n",
      "\n",
      "\n",
      "=====company_profile=====\n",
      "['bonus', 'referral', 'candidates', 'significant', 'refined', 'resources', 'signing', 'candidate', 'anyperk', 'automatically', 'behalf', 'cleaning', 'click', 'corporate', 'directly', 'discount', 'enlarge', 'expenditure', 'formthank', 'forward']\n",
      "\n",
      "\n",
      "\n",
      "=====description=====\n",
      "['equipment', 'performs', 'maintenance', 'control', 'required', 'installs', 'troubleshoots', 'identifies', 'member', 'instrumentation', 'motor', 'follows', 'work', 'plant', 'electrical', 'environmental', 'power', 'team', 'coordinate', 'record']\n",
      "\n",
      "\n",
      "\n",
      "=====requirements=====\n",
      "['demonstrated', 'equipment', 'plant', 'control', 'including', 'electrical', 'power', 'ability', 'environment', 'systems', 'qualificationsknowledge', 'calibrate', 'cem', 'controllers', 'emission', 'generators', 'physic', 'programmable', 'transformers', 'analytic']\n",
      "\n",
      "\n",
      "\n",
      "=====benefits=====\n",
      "['career', 'employee', 'benefitswhat', 'renound', 'goalsqualified', 'phone_70128aad0c118273b0c2198a08d528591b932924e165b6a8d1272a6f9e2763d1', 'company', 'companysignificant', 'cultureworld', 'fundannual', 'increasesannual', 'invest', 'overall', 'package100', 'packageannual', 'sound', 'structureopportunity', 'leverage', 'living', 'promote']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in refined_only_fake_text.columns[:-1]:\n",
    "    print(f'====={c}=====')\n",
    "    print(get_unique_keywords(refined_only_fake_text, c, 20, d_type='fake', ngram=(1,1)))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====title=====\n",
      "['intern', 'marketing', '00', '000', '02', '0dq', '0rg', '0tz', '10', '100', '100k', '1099', '10k', '10x', '11', '12', '12hr', '13', '13th', '14']\n",
      "\n",
      "\n",
      "\n",
      "=====company_profile=====\n",
      "['food', 'food52', 'cooking', 'connect', 'york', 'new', 'home', 'batali', 'beard', 'contributor', 'cooks', 'danny', 'gwyneth', 'iacp', 'mario', 'meyer', 'npr', 'paltrow', 'pando', 'random']\n",
      "\n",
      "\n",
      "\n",
      "=====description=====\n",
      "['affiliate', 'food52', 'inquiriessupporting', 'meetingsworking', 'neededhelping', 'programassisting', 'systemsresearching', 'repackaging', 'reproducing', 'sitesupporting', 'huffington', 'buzzfeed', 'beard', 'crowd', 'provisions', 'james', 'unpaid', 'content', 'editors', 'sourced']\n",
      "\n",
      "\n",
      "\n",
      "=====requirements=====\n",
      "['cooking', 'food52', 'big', 'aestheticloves', 'counts', 'pinterestloves', 'seasonsmeticulous', 'smallinterested', 'dishes', 'forwardthinks', 'themcheerful', 'delighted', 'gritty', 'maddened', 'nitty', 'appreciates', 'broken', 'juggler', 'pressureexcellent', 'typo']\n",
      "\n",
      "\n",
      "\n",
      "=====benefits=====\n",
      "['00', '000', '0000', '0001pt', '000aed', '000annual', '000apply', '000applying', '000benefits', '000bonus', '000cash', '000commission', '000company', '000equity', '000health', '000high', '000highly', '000hours', '000how', '000incentivised']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in refined_only_real_text.columns[:-1]:\n",
    "    print(f'====={c}=====')\n",
    "    print(get_unique_keywords(refined_only_real_text, c, 20,  d_type='real', ngram=(1,1)))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_by = 100\n",
    "number_of_parts = len(only_fake_text)//div_by + 1\n",
    "\n",
    "for i in range(number_of_parts):\n",
    "    only_fake_text.iloc[:i*div_by, :].to_json(f'{BASE_DIR}/data/only_fake_text_{i}.json', indent=4, orient='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to check the magnitue of string of numbers in the texts\n",
    "\n",
    "def collect_numbers(df, col):\n",
    "    numbers = []\n",
    "    target = df[col]\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        numbers.extend(re.findall(r'[0-9][0-9,.]+', target.iloc[i]))\n",
    "        \n",
    "    for i in range(len(numbers)):\n",
    "        \n",
    "        if not '.' in numbers[i]:\n",
    "            numbers[i] = int(numbers[i].replace(',', ''))\n",
    "            \n",
    "        else:\n",
    "            numbers[i] = ''.join(numbers[i].split('.')[:-1])\n",
    "            numbers[i] = float(numbers[i].replace(',', ''))\n",
    "        \n",
    "    return numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====title=====\n",
      "mean:  24756.87479674797\n",
      "std:  56034.21728161342\n",
      "max:  175000.0\n",
      "min:  15.0\n",
      "median:  200.0\n",
      "\n",
      "\n",
      "\n",
      "=====company_profile=====\n",
      "mean:  17283305.589181285\n",
      "std:  114872796.26757514\n",
      "max:  916884407.0\n",
      "min:  3.0\n",
      "median:  500.0\n",
      "\n",
      "\n",
      "\n",
      "=====description=====\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '6.5.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m refined_only_fake_text\u001b[39m.\u001b[39mcolumns[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[0;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m=====\u001b[39m\u001b[39m{\u001b[39;00mc\u001b[39m}\u001b[39;00m\u001b[39m=====\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     ns \u001b[39m=\u001b[39m collect_numbers(refined_only_fake_text, c)\n\u001b[0;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmean: \u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mmean(ns))\n\u001b[0;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mstd: \u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mstd(ns))\n",
      "Cell \u001b[1;32mIn[64], line 16\u001b[0m, in \u001b[0;36mcollect_numbers\u001b[1;34m(df, col)\u001b[0m\n\u001b[0;32m     13\u001b[0m         numbers[i] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(numbers[i]\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     15\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 16\u001b[0m         numbers[i] \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39;49m(numbers[i]\u001b[39m.\u001b[39;49mreplace(\u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m numbers\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '6.5.'"
     ]
    }
   ],
   "source": [
    "for c in refined_only_fake_text.columns[:-1]:\n",
    "    print(f'====={c}=====')\n",
    "    ns = collect_numbers(refined_only_fake_text, c)\n",
    "    \n",
    "    print(\"mean: \", np.mean(ns))\n",
    "    print(\"std: \", np.std(ns))\n",
    "    print(\"max: \", np.max(ns))\n",
    "    print(\"min: \", np.min(ns))\n",
    "    print(\"median: \", np.median(ns))\n",
    "\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = collect_numbers(only, 'benefits')\n",
    "print(\"mean: \", np.mean(ns))\n",
    "print(\"std: \", np.std(ns))\n",
    "print(\"max: \", np.max(ns))\n",
    "print(\"min: \", np.min(ns))\n",
    "print(\"median: \", np.median(ns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
